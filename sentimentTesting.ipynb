{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('vsenv': conda)",
   "display_name": "Python 3.8.5 64-bit ('vsenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1ee30a202add5215adcbb767012aec8477a9c484e522cef3494d0f72d70b8c03"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import nltk\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from bson.objectid import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost',27017)\n",
    "db = client.db\n",
    "comments = db.comments\n",
    "companies = db.companylist\n",
    "matches = db.matches\n",
    "chunks = db.chunks\n",
    "\n",
    "df = pd.DataFrame.from_records(comments.find({'Label':{'$ne': 0}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"nltk.download('vader_lexicon')\\nsir = SentimentIntensityAnalyzer()\\nsent = df.loc[77,'Title'] + ' ' + df.loc[77,'Body']\\nsir.polarity_scores(sent)['compound']\\ncount = 0\\n\\nfor i in range(len(df)):\\n    sent = df.loc[i,'Title'] + ' ' + df.loc[i,'Body']\\n    score = sir.polarity_scores(sent)['compound']\\n    if (score < 0 and df.loc[i, 'Label'] == 'bullish') or (score > 0 and df.loc[i,'Label'] == 'bearish'):\\n        print(str(score) + ':' + df.loc[i, 'Label'])\\n        count = count + 1\""
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "'''nltk.download('vader_lexicon')\n",
    "sir = SentimentIntensityAnalyzer()\n",
    "sent = df.loc[77,'Title'] + ' ' + df.loc[77,'Body']\n",
    "sir.polarity_scores(sent)['compound']\n",
    "count = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    sent = df.loc[i,'Title'] + ' ' + df.loc[i,'Body']\n",
    "    score = sir.polarity_scores(sent)['compound']\n",
    "    if (score < 0 and df.loc[i, 'Label'] == 'bullish') or (score > 0 and df.loc[i,'Label'] == 'bearish'):\n",
    "        print(str(score) + ':' + df.loc[i, 'Label'])\n",
    "        count = count + 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf = pd.DataFrame(columns = ['Label', 'Text'])\n",
    "labelDf['Text'] = df['Body']\n",
    "labelDf['Label'] = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf['Label'] = '__label__' + labelDf['Label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf = labelDf.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf.iloc[0: int(len(labelDf)*0.8)].to_csv('data/train.csv', sep = '\\t', index = False, header = False)\n",
    "labelDf.iloc[int(len(labelDf)*0.8): int(len(labelDf)*0.9)].to_csv('data/test.csv', sep = '\\t', index = False, header = False)\n",
    "labelDf.iloc[int(len(labelDf)*0.9): ].to_csv('data/dev.csv', sep = '\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-11-01 20:32:32,804 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:32:32,805 Corpus: \"Corpus: 1246 train + 156 dev + 156 test sentences\"\n",
      "2020-11-01 20:32:32,809 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:32:32,811 Parameters:\n",
      "2020-11-01 20:32:32,814  - learning_rate: \"0.1\"\n",
      "\n",
      "2020-11-01 20:32:32,817  - mini_batch_size: \"32\"\n",
      "2020-11-01 20:32:32,819  - patience: \"3\"\n",
      "2020-11-01 20:32:32,820  - anneal_factor: \"0.5\"\n",
      "2020-11-01 20:32:32,822  - max_epochs: \"10\"\n",
      "2020-11-01 20:32:32,824  - shuffle: \"True\"\n",
      "2020-11-01 20:32:32,825  - train_with_dev: \"False\"\n",
      "2020-11-01 20:32:32,826  - batch_growth_annealing: \"False\"\n",
      "2020-11-01 20:32:32,827 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:32:32,829 Model training base path: \"model\"\n",
      "2020-11-01 20:32:32,830 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:32:32,834 Device: cpu\n",
      "2020-11-01 20:32:32,835 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:32:32,836 Embeddings storage mode: cpu\n",
      "2020-11-01 20:32:32,839 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:32:40,749 epoch 1 - iter 3/39 - loss 1.10512507 - samples/sec: 13.59 - lr: 0.100000\n",
      "2020-11-01 20:32:45,277 epoch 1 - iter 6/39 - loss 1.10283420 - samples/sec: 21.31 - lr: 0.100000\n",
      "2020-11-01 20:32:53,732 epoch 1 - iter 9/39 - loss 1.09850098 - samples/sec: 11.36 - lr: 0.100000\n",
      "2020-11-01 20:33:00,141 epoch 1 - iter 12/39 - loss 1.10121947 - samples/sec: 14.99 - lr: 0.100000\n",
      "2020-11-01 20:33:05,762 epoch 1 - iter 15/39 - loss 1.10073434 - samples/sec: 17.12 - lr: 0.100000\n",
      "2020-11-01 20:33:16,042 epoch 1 - iter 18/39 - loss 1.10467022 - samples/sec: 9.35 - lr: 0.100000\n",
      "2020-11-01 20:33:20,569 epoch 1 - iter 21/39 - loss 1.10135816 - samples/sec: 21.21 - lr: 0.100000\n",
      "2020-11-01 20:33:25,882 epoch 1 - iter 24/39 - loss 1.09697249 - samples/sec: 18.11 - lr: 0.100000\n",
      "2020-11-01 20:33:35,912 epoch 1 - iter 27/39 - loss 1.10095575 - samples/sec: 9.57 - lr: 0.100000\n",
      "2020-11-01 20:33:40,631 epoch 1 - iter 30/39 - loss 1.09904314 - samples/sec: 20.41 - lr: 0.100000\n",
      "2020-11-01 20:33:49,275 epoch 1 - iter 33/39 - loss 1.10003673 - samples/sec: 11.11 - lr: 0.100000\n",
      "2020-11-01 20:33:53,851 epoch 1 - iter 36/39 - loss 1.09845705 - samples/sec: 21.02 - lr: 0.100000\n",
      "2020-11-01 20:33:59,820 epoch 1 - iter 39/39 - loss 1.09771336 - samples/sec: 16.09 - lr: 0.100000\n",
      "2020-11-01 20:33:59,913 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:33:59,914 EPOCH 1 done: loss 1.0977 - lr 0.1000000\n",
      "2020-11-01 20:34:09,916 DEV : loss 1.0662189722061157 - score 0.4423\n",
      "2020-11-01 20:34:10,013 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-01 20:34:13,151 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:34:24,416 epoch 2 - iter 3/39 - loss 1.09793667 - samples/sec: 10.07 - lr: 0.100000\n",
      "2020-11-01 20:34:34,332 epoch 2 - iter 6/39 - loss 1.09495666 - samples/sec: 9.70 - lr: 0.100000\n",
      "2020-11-01 20:34:39,747 epoch 2 - iter 9/39 - loss 1.09130388 - samples/sec: 17.74 - lr: 0.100000\n",
      "2020-11-01 20:34:44,095 epoch 2 - iter 12/39 - loss 1.08655631 - samples/sec: 22.14 - lr: 0.100000\n",
      "2020-11-01 20:34:51,563 epoch 2 - iter 15/39 - loss 1.08280606 - samples/sec: 12.86 - lr: 0.100000\n",
      "2020-11-01 20:34:58,925 epoch 2 - iter 18/39 - loss 1.07182839 - samples/sec: 13.08 - lr: 0.100000\n",
      "2020-11-01 20:35:05,967 epoch 2 - iter 21/39 - loss 1.07208492 - samples/sec: 13.64 - lr: 0.100000\n",
      "2020-11-01 20:35:16,689 epoch 2 - iter 24/39 - loss 1.07779082 - samples/sec: 8.97 - lr: 0.100000\n",
      "2020-11-01 20:35:22,203 epoch 2 - iter 27/39 - loss 1.08148919 - samples/sec: 17.41 - lr: 0.100000\n",
      "2020-11-01 20:35:30,891 epoch 2 - iter 30/39 - loss 1.07816744 - samples/sec: 11.07 - lr: 0.100000\n",
      "2020-11-01 20:35:35,734 epoch 2 - iter 33/39 - loss 1.07924756 - samples/sec: 19.83 - lr: 0.100000\n",
      "2020-11-01 20:35:40,426 epoch 2 - iter 36/39 - loss 1.07976774 - samples/sec: 20.55 - lr: 0.100000\n",
      "2020-11-01 20:35:46,117 epoch 2 - iter 39/39 - loss 1.07799833 - samples/sec: 16.87 - lr: 0.100000\n",
      "2020-11-01 20:35:46,221 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:35:46,274 EPOCH 2 done: loss 1.0780 - lr 0.1000000\n",
      "2020-11-01 20:35:59,543 DEV : loss 1.0878649950027466 - score 0.3782\n",
      "2020-11-01 20:35:59,674 BAD EPOCHS (no improvement): 1\n",
      "2020-11-01 20:35:59,677 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:36:06,703 epoch 3 - iter 3/39 - loss 1.02554619 - samples/sec: 18.05 - lr: 0.100000\n",
      "2020-11-01 20:36:15,102 epoch 3 - iter 6/39 - loss 1.05183432 - samples/sec: 11.45 - lr: 0.100000\n",
      "2020-11-01 20:36:26,713 epoch 3 - iter 9/39 - loss 1.05690173 - samples/sec: 8.27 - lr: 0.100000\n",
      "2020-11-01 20:36:30,842 epoch 3 - iter 12/39 - loss 1.04206991 - samples/sec: 23.31 - lr: 0.100000\n",
      "2020-11-01 20:36:36,272 epoch 3 - iter 15/39 - loss 1.04847149 - samples/sec: 17.68 - lr: 0.100000\n",
      "2020-11-01 20:36:47,983 epoch 3 - iter 18/39 - loss 1.05185999 - samples/sec: 8.21 - lr: 0.100000\n",
      "2020-11-01 20:36:56,311 epoch 3 - iter 21/39 - loss 1.06149868 - samples/sec: 11.53 - lr: 0.100000\n",
      "2020-11-01 20:37:01,356 epoch 3 - iter 24/39 - loss 1.06063344 - samples/sec: 19.07 - lr: 0.100000\n",
      "2020-11-01 20:37:06,151 epoch 3 - iter 27/39 - loss 1.05805554 - samples/sec: 20.02 - lr: 0.100000\n",
      "2020-11-01 20:37:11,086 epoch 3 - iter 30/39 - loss 1.05324933 - samples/sec: 19.51 - lr: 0.100000\n",
      "2020-11-01 20:37:17,674 epoch 3 - iter 33/39 - loss 1.05186366 - samples/sec: 14.57 - lr: 0.100000\n",
      "2020-11-01 20:37:22,667 epoch 3 - iter 36/39 - loss 1.05040375 - samples/sec: 19.26 - lr: 0.100000\n",
      "2020-11-01 20:37:27,024 epoch 3 - iter 39/39 - loss 1.05703940 - samples/sec: 22.04 - lr: 0.100000\n",
      "2020-11-01 20:37:27,130 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:37:27,177 EPOCH 3 done: loss 1.0570 - lr 0.1000000\n",
      "2020-11-01 20:37:39,203 DEV : loss 1.075381875038147 - score 0.4423\n",
      "2020-11-01 20:37:39,314 BAD EPOCHS (no improvement): 2\n",
      "2020-11-01 20:37:39,318 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:37:51,667 epoch 4 - iter 3/39 - loss 1.03103900 - samples/sec: 11.61 - lr: 0.100000\n",
      "2020-11-01 20:38:01,120 epoch 4 - iter 6/39 - loss 1.03815498 - samples/sec: 10.16 - lr: 0.100000\n",
      "2020-11-01 20:38:07,442 epoch 4 - iter 9/39 - loss 1.02855129 - samples/sec: 15.20 - lr: 0.100000\n",
      "2020-11-01 20:38:12,369 epoch 4 - iter 12/39 - loss 1.02447823 - samples/sec: 19.49 - lr: 0.100000\n",
      "2020-11-01 20:38:24,678 epoch 4 - iter 15/39 - loss 1.03589290 - samples/sec: 7.81 - lr: 0.100000\n",
      "2020-11-01 20:38:29,728 epoch 4 - iter 18/39 - loss 1.03758707 - samples/sec: 19.04 - lr: 0.100000\n",
      "2020-11-01 20:38:41,011 epoch 4 - iter 21/39 - loss 1.03366297 - samples/sec: 8.52 - lr: 0.100000\n",
      "2020-11-01 20:38:49,740 epoch 4 - iter 24/39 - loss 1.02598439 - samples/sec: 11.01 - lr: 0.100000\n",
      "2020-11-01 20:38:54,345 epoch 4 - iter 27/39 - loss 1.03611534 - samples/sec: 20.88 - lr: 0.100000\n",
      "2020-11-01 20:38:59,337 epoch 4 - iter 30/39 - loss 1.03018925 - samples/sec: 19.25 - lr: 0.100000\n",
      "2020-11-01 20:39:04,863 epoch 4 - iter 33/39 - loss 1.02985648 - samples/sec: 20.36 - lr: 0.100000\n",
      "2020-11-01 20:39:08,615 epoch 4 - iter 36/39 - loss 1.03257364 - samples/sec: 25.64 - lr: 0.100000\n",
      "2020-11-01 20:39:12,939 epoch 4 - iter 39/39 - loss 1.02674366 - samples/sec: 22.24 - lr: 0.100000\n",
      "2020-11-01 20:39:13,069 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:39:13,073 EPOCH 4 done: loss 1.0267 - lr 0.1000000\n",
      "2020-11-01 20:39:24,351 DEV : loss 1.0330644845962524 - score 0.4615\n",
      "2020-11-01 20:39:24,451 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-01 20:39:27,576 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:39:33,710 epoch 5 - iter 3/39 - loss 0.95929992 - samples/sec: 21.94 - lr: 0.100000\n",
      "2020-11-01 20:39:38,793 epoch 5 - iter 6/39 - loss 0.99835539 - samples/sec: 18.97 - lr: 0.100000\n",
      "2020-11-01 20:39:45,079 epoch 5 - iter 9/39 - loss 0.99957807 - samples/sec: 15.27 - lr: 0.100000\n",
      "2020-11-01 20:39:48,868 epoch 5 - iter 12/39 - loss 1.02147163 - samples/sec: 25.42 - lr: 0.100000\n",
      "2020-11-01 20:39:57,602 epoch 5 - iter 15/39 - loss 1.02859900 - samples/sec: 10.99 - lr: 0.100000\n",
      "2020-11-01 20:40:03,499 epoch 5 - iter 18/39 - loss 1.03343586 - samples/sec: 16.32 - lr: 0.100000\n",
      "2020-11-01 20:40:10,740 epoch 5 - iter 21/39 - loss 1.03982999 - samples/sec: 13.26 - lr: 0.100000\n",
      "2020-11-01 20:40:17,605 epoch 5 - iter 24/39 - loss 1.04229444 - samples/sec: 14.01 - lr: 0.100000\n",
      "2020-11-01 20:40:25,510 epoch 5 - iter 27/39 - loss 1.03994820 - samples/sec: 12.15 - lr: 0.100000\n",
      "2020-11-01 20:40:31,157 epoch 5 - iter 30/39 - loss 1.04213245 - samples/sec: 17.04 - lr: 0.100000\n",
      "2020-11-01 20:40:38,964 epoch 5 - iter 33/39 - loss 1.04002252 - samples/sec: 12.30 - lr: 0.100000\n",
      "2020-11-01 20:40:43,141 epoch 5 - iter 36/39 - loss 1.03891034 - samples/sec: 23.04 - lr: 0.100000\n",
      "2020-11-01 20:40:58,584 epoch 5 - iter 39/39 - loss 1.03388232 - samples/sec: 6.22 - lr: 0.100000\n",
      "2020-11-01 20:40:58,705 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:40:58,769 EPOCH 5 done: loss 1.0339 - lr 0.1000000\n",
      "2020-11-01 20:41:09,761 DEV : loss 1.043900728225708 - score 0.5128\n",
      "2020-11-01 20:41:09,872 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-01 20:41:30,772 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:41:38,806 epoch 6 - iter 3/39 - loss 1.00752676 - samples/sec: 19.60 - lr: 0.100000\n",
      "2020-11-01 20:41:44,925 epoch 6 - iter 6/39 - loss 1.04647988 - samples/sec: 15.71 - lr: 0.100000\n",
      "2020-11-01 20:41:48,875 epoch 6 - iter 9/39 - loss 1.04199886 - samples/sec: 24.34 - lr: 0.100000\n",
      "2020-11-01 20:41:53,573 epoch 6 - iter 12/39 - loss 1.03512104 - samples/sec: 20.46 - lr: 0.100000\n",
      "2020-11-01 20:42:03,696 epoch 6 - iter 15/39 - loss 1.04028925 - samples/sec: 9.49 - lr: 0.100000\n",
      "2020-11-01 20:42:15,916 epoch 6 - iter 18/39 - loss 1.03404252 - samples/sec: 7.86 - lr: 0.100000\n",
      "2020-11-01 20:42:22,903 epoch 6 - iter 21/39 - loss 1.03067267 - samples/sec: 13.76 - lr: 0.100000\n",
      "2020-11-01 20:42:30,351 epoch 6 - iter 24/39 - loss 1.02880282 - samples/sec: 12.91 - lr: 0.100000\n",
      "2020-11-01 20:42:35,322 epoch 6 - iter 27/39 - loss 1.02996384 - samples/sec: 22.58 - lr: 0.100000\n",
      "2020-11-01 20:42:40,467 epoch 6 - iter 30/39 - loss 1.02195662 - samples/sec: 18.67 - lr: 0.100000\n",
      "2020-11-01 20:42:47,492 epoch 6 - iter 33/39 - loss 1.02416978 - samples/sec: 13.68 - lr: 0.100000\n",
      "2020-11-01 20:42:53,000 epoch 6 - iter 36/39 - loss 1.03187535 - samples/sec: 17.43 - lr: 0.100000\n",
      "2020-11-01 20:42:59,660 epoch 6 - iter 39/39 - loss 1.03235250 - samples/sec: 14.44 - lr: 0.100000\n",
      "2020-11-01 20:42:59,877 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:42:59,908 EPOCH 6 done: loss 1.0324 - lr 0.1000000\n",
      "2020-11-01 20:43:11,123 DEV : loss 1.058959722518921 - score 0.4679\n",
      "2020-11-01 20:43:11,299 BAD EPOCHS (no improvement): 1\n",
      "2020-11-01 20:43:11,301 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:43:23,208 epoch 7 - iter 3/39 - loss 0.98735762 - samples/sec: 9.69 - lr: 0.100000\n",
      "2020-11-01 20:43:31,554 epoch 7 - iter 6/39 - loss 1.00913442 - samples/sec: 11.56 - lr: 0.100000\n",
      "2020-11-01 20:43:38,267 epoch 7 - iter 9/39 - loss 1.02322633 - samples/sec: 14.30 - lr: 0.100000\n",
      "2020-11-01 20:43:48,358 epoch 7 - iter 12/39 - loss 1.01811165 - samples/sec: 9.53 - lr: 0.100000\n",
      "2020-11-01 20:43:56,685 epoch 7 - iter 15/39 - loss 1.01340398 - samples/sec: 11.62 - lr: 0.100000\n",
      "2020-11-01 20:44:01,255 epoch 7 - iter 18/39 - loss 1.01828627 - samples/sec: 21.06 - lr: 0.100000\n",
      "2020-11-01 20:44:07,780 epoch 7 - iter 21/39 - loss 1.01743797 - samples/sec: 14.71 - lr: 0.100000\n",
      "2020-11-01 20:44:12,390 epoch 7 - iter 24/39 - loss 1.02380076 - samples/sec: 20.88 - lr: 0.100000\n",
      "2020-11-01 20:44:17,433 epoch 7 - iter 27/39 - loss 1.02000149 - samples/sec: 19.04 - lr: 0.100000\n",
      "2020-11-01 20:44:23,237 epoch 7 - iter 30/39 - loss 1.01587814 - samples/sec: 16.57 - lr: 0.100000\n",
      "2020-11-01 20:44:27,594 epoch 7 - iter 33/39 - loss 1.01095148 - samples/sec: 22.04 - lr: 0.100000\n",
      "2020-11-01 20:44:34,289 epoch 7 - iter 36/39 - loss 1.00513819 - samples/sec: 14.36 - lr: 0.100000\n",
      "2020-11-01 20:44:41,479 epoch 7 - iter 39/39 - loss 1.01015917 - samples/sec: 13.35 - lr: 0.100000\n",
      "2020-11-01 20:44:41,599 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:44:41,683 EPOCH 7 done: loss 1.0102 - lr 0.1000000\n",
      "2020-11-01 20:44:53,701 DEV : loss 1.0392714738845825 - score 0.4167\n",
      "2020-11-01 20:44:53,811 BAD EPOCHS (no improvement): 2\n",
      "2020-11-01 20:44:53,813 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:45:04,078 epoch 8 - iter 3/39 - loss 1.02352597 - samples/sec: 16.58 - lr: 0.100000\n",
      "2020-11-01 20:45:09,332 epoch 8 - iter 6/39 - loss 1.03715933 - samples/sec: 18.31 - lr: 0.100000\n",
      "2020-11-01 20:45:14,890 epoch 8 - iter 9/39 - loss 1.01295590 - samples/sec: 17.28 - lr: 0.100000\n",
      "2020-11-01 20:45:25,596 epoch 8 - iter 12/39 - loss 1.01427875 - samples/sec: 8.98 - lr: 0.100000\n",
      "2020-11-01 20:45:30,035 epoch 8 - iter 15/39 - loss 1.01326834 - samples/sec: 21.63 - lr: 0.100000\n",
      "2020-11-01 20:45:40,112 epoch 8 - iter 18/39 - loss 1.01676739 - samples/sec: 9.54 - lr: 0.100000\n",
      "2020-11-01 20:45:45,920 epoch 8 - iter 21/39 - loss 1.02125724 - samples/sec: 19.33 - lr: 0.100000\n",
      "2020-11-01 20:45:51,550 epoch 8 - iter 24/39 - loss 1.01656302 - samples/sec: 17.08 - lr: 0.100000\n",
      "2020-11-01 20:45:58,888 epoch 8 - iter 27/39 - loss 1.01406585 - samples/sec: 13.10 - lr: 0.100000\n",
      "2020-11-01 20:46:11,448 epoch 8 - iter 30/39 - loss 1.01141964 - samples/sec: 7.65 - lr: 0.100000\n",
      "2020-11-01 20:46:16,023 epoch 8 - iter 33/39 - loss 1.00640034 - samples/sec: 21.00 - lr: 0.100000\n",
      "2020-11-01 20:46:21,934 epoch 8 - iter 36/39 - loss 1.00658850 - samples/sec: 16.26 - lr: 0.100000\n",
      "2020-11-01 20:46:26,005 epoch 8 - iter 39/39 - loss 1.00751735 - samples/sec: 23.59 - lr: 0.100000\n",
      "2020-11-01 20:46:26,131 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:46:26,233 EPOCH 8 done: loss 1.0075 - lr 0.1000000\n",
      "2020-11-01 20:46:36,946 DEV : loss 0.9996782541275024 - score 0.5449\n",
      "2020-11-01 20:46:37,047 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-01 20:46:40,210 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:46:46,957 epoch 9 - iter 3/39 - loss 0.94033597 - samples/sec: 20.29 - lr: 0.100000\n",
      "2020-11-01 20:46:53,065 epoch 9 - iter 6/39 - loss 0.98349208 - samples/sec: 15.75 - lr: 0.100000\n",
      "2020-11-01 20:47:00,141 epoch 9 - iter 9/39 - loss 0.98174494 - samples/sec: 13.58 - lr: 0.100000\n",
      "2020-11-01 20:47:05,814 epoch 9 - iter 12/39 - loss 0.97450410 - samples/sec: 16.96 - lr: 0.100000\n",
      "2020-11-01 20:47:15,038 epoch 9 - iter 15/39 - loss 0.98730774 - samples/sec: 10.41 - lr: 0.100000\n",
      "2020-11-01 20:47:25,384 epoch 9 - iter 18/39 - loss 0.98698116 - samples/sec: 9.30 - lr: 0.100000\n",
      "2020-11-01 20:47:29,274 epoch 9 - iter 21/39 - loss 0.99379544 - samples/sec: 24.68 - lr: 0.100000\n",
      "2020-11-01 20:47:34,563 epoch 9 - iter 24/39 - loss 0.99118241 - samples/sec: 18.19 - lr: 0.100000\n",
      "2020-11-01 20:47:42,323 epoch 9 - iter 27/39 - loss 0.99005577 - samples/sec: 12.37 - lr: 0.100000\n",
      "2020-11-01 20:47:46,270 epoch 9 - iter 30/39 - loss 0.99650572 - samples/sec: 24.51 - lr: 0.100000\n",
      "2020-11-01 20:47:57,833 epoch 9 - iter 33/39 - loss 0.99761134 - samples/sec: 8.30 - lr: 0.100000\n",
      "2020-11-01 20:48:06,035 epoch 9 - iter 36/39 - loss 1.00070480 - samples/sec: 11.71 - lr: 0.100000\n",
      "2020-11-01 20:48:10,735 epoch 9 - iter 39/39 - loss 0.99575309 - samples/sec: 20.43 - lr: 0.100000\n",
      "2020-11-01 20:48:10,854 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:48:10,924 EPOCH 9 done: loss 0.9958 - lr 0.1000000\n",
      "2020-11-01 20:48:23,065 DEV : loss 1.0420639514923096 - score 0.4744\n",
      "2020-11-01 20:48:23,190 BAD EPOCHS (no improvement): 1\n",
      "2020-11-01 20:48:23,192 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:48:32,667 epoch 10 - iter 3/39 - loss 0.92596471 - samples/sec: 20.19 - lr: 0.100000\n",
      "2020-11-01 20:48:39,108 epoch 10 - iter 6/39 - loss 0.96160586 - samples/sec: 14.94 - lr: 0.100000\n",
      "2020-11-01 20:48:44,043 epoch 10 - iter 9/39 - loss 1.00056786 - samples/sec: 19.45 - lr: 0.100000\n",
      "2020-11-01 20:48:48,638 epoch 10 - iter 12/39 - loss 1.00220789 - samples/sec: 20.99 - lr: 0.100000\n",
      "2020-11-01 20:48:53,385 epoch 10 - iter 15/39 - loss 0.98967729 - samples/sec: 20.23 - lr: 0.100000\n",
      "2020-11-01 20:48:59,575 epoch 10 - iter 18/39 - loss 0.99979130 - samples/sec: 17.71 - lr: 0.100000\n",
      "2020-11-01 20:49:06,841 epoch 10 - iter 21/39 - loss 0.99939198 - samples/sec: 13.21 - lr: 0.100000\n",
      "2020-11-01 20:49:15,062 epoch 10 - iter 24/39 - loss 1.00578332 - samples/sec: 11.70 - lr: 0.100000\n",
      "2020-11-01 20:49:24,245 epoch 10 - iter 27/39 - loss 1.00616148 - samples/sec: 10.46 - lr: 0.100000\n",
      "2020-11-01 20:49:33,500 epoch 10 - iter 30/39 - loss 1.00833160 - samples/sec: 10.39 - lr: 0.100000\n",
      "2020-11-01 20:49:39,685 epoch 10 - iter 33/39 - loss 0.99839188 - samples/sec: 15.52 - lr: 0.100000\n",
      "2020-11-01 20:49:45,767 epoch 10 - iter 36/39 - loss 0.99957592 - samples/sec: 15.81 - lr: 0.100000\n",
      "2020-11-01 20:49:57,068 epoch 10 - iter 39/39 - loss 0.99866501 - samples/sec: 8.50 - lr: 0.100000\n",
      "2020-11-01 20:49:57,254 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:49:57,279 EPOCH 10 done: loss 0.9987 - lr 0.1000000\n",
      "2020-11-01 20:50:09,828 DEV : loss 1.0387905836105347 - score 0.4936\n",
      "2020-11-01 20:50:09,937 BAD EPOCHS (no improvement): 2\n",
      "2020-11-01 20:50:13,528 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-01 20:50:13,529 Testing using best model ...\n",
      "2020-11-01 20:50:13,530 loading file model/best-model.pt\n",
      "2020-11-01 20:50:28,163 \t0.4551\n",
      "2020-11-01 20:50:28,165 \n",
      "Results:\n",
      "- F-score (micro) 0.4551\n",
      "- F-score (macro) 0.3942\n",
      "- Accuracy 0.4551\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.4587    0.8065    0.5848        62\n",
      "     bullish     0.4800    0.2105    0.2927        57\n",
      "     bearish     0.4091    0.2432    0.3051        37\n",
      "\n",
      "   micro avg     0.4551    0.4551    0.4551       156\n",
      "   macro avg     0.4493    0.4201    0.3942       156\n",
      "weighted avg     0.4547    0.4551    0.4117       156\n",
      " samples avg     0.4551    0.4551    0.4551       156\n",
      "\n",
      "2020-11-01 20:50:28,166 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'test_score': 0.4551,\n",
       " 'dev_score_history': [0.4423,\n",
       "  0.3782,\n",
       "  0.4423,\n",
       "  0.4615,\n",
       "  0.5128,\n",
       "  0.4679,\n",
       "  0.4167,\n",
       "  0.5449,\n",
       "  0.4744,\n",
       "  0.4936],\n",
       " 'train_loss_history': [1.0977133634762886,\n",
       "  1.0779983324882312,\n",
       "  1.0570394029984107,\n",
       "  1.0267436580780225,\n",
       "  1.0338823245121882,\n",
       "  1.032352504057762,\n",
       "  1.0101591654312916,\n",
       "  1.0075173484973419,\n",
       "  0.9957530926435422,\n",
       "  0.9986650133744265],\n",
       " 'dev_loss_history': [1.0662189722061157,\n",
       "  1.0878649950027466,\n",
       "  1.075381875038147,\n",
       "  1.0330644845962524,\n",
       "  1.043900728225708,\n",
       "  1.058959722518921,\n",
       "  1.0392714738845825,\n",
       "  0.9996782541275024,\n",
       "  1.0420639514923096,\n",
       "  1.0387905836105347]}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "corpus = ClassificationCorpus(Path('data/'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
    "word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('model/', max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-11-02 18:11:40,431 loading file model/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier.load('model/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'value': 'neutral', 'confidence': 0.46689432859420776}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "sentence = Sentence('stonks are flat')\n",
    "classifier.predict(sentence)\n",
    "sentence.labels[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(db.threads.find({'Label':{'$ne': 0}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "[bullish (0.4264)]\n",
      "[bearish (0.4014)]\n",
      "[bullish (0.3883)]\n",
      "[neutral (0.361)]\n",
      "[neutral (0.5081)]\n",
      "[neutral (0.4587)]\n",
      "[neutral (0.5477)]\n",
      "[neutral (0.6959)]\n",
      "[neutral (0.4369)]\n",
      "1\n",
      "[neutral (0.4986)]\n",
      "[bullish (0.4714)]\n",
      "[neutral (0.3696)]\n",
      "[bullish (0.469)]\n",
      "[bearish (0.4374)]\n",
      "[bullish (0.583)]\n",
      "[neutral (0.5378)]\n",
      "[neutral (0.4533)]\n",
      "[bullish (0.3903)]\n",
      "[neutral (0.4696)]\n",
      "[neutral (0.5345)]\n",
      "[bullish (0.3929)]\n",
      "[bearish (0.4591)]\n",
      "[bearish (0.4687)]\n",
      "[bullish (0.6079)]\n",
      "[bullish (0.5661)]\n",
      "[bullish (0.4594)]\n",
      "[neutral (0.4239)]\n",
      "[bullish (0.3992)]\n",
      "[bullish (0.3883)]\n",
      "[bullish (0.3863)]\n",
      "[neutral (0.4566)]\n",
      "[bearish (0.4039)]\n",
      "[neutral (0.4199)]\n",
      "[neutral (0.6094)]\n",
      "[neutral (0.6532)]\n",
      "[neutral (0.3901)]\n",
      "2\n",
      "[neutral (0.4543)]\n",
      "[neutral (0.4557)]\n",
      "[bullish (0.485)]\n",
      "[bearish (0.4268)]\n",
      "[bullish (0.4372)]\n",
      "[neutral (0.3578)]\n",
      "[neutral (0.6233)]\n",
      "[bearish (0.4631)]\n",
      "[neutral (0.3811)]\n",
      "[neutral (0.5655)]\n",
      "[neutral (0.5073)]\n",
      "[neutral (0.3623)]\n",
      "[neutral (0.5085)]\n",
      "[neutral (0.5124)]\n",
      "[neutral (0.6283)]\n",
      "[neutral (0.4525)]\n",
      "3\n",
      "[bullish (0.542)]\n",
      "[neutral (0.5325)]\n",
      "[neutral (0.3989)]\n",
      "[bullish (0.3428)]\n",
      "[neutral (0.7404)]\n",
      "[neutral (0.5306)]\n",
      "[neutral (0.6469)]\n",
      "[neutral (0.4649)]\n",
      "[neutral (0.4691)]\n",
      "[bullish (0.3546)]\n",
      "[bullish (0.3604)]\n",
      "[neutral (0.3904)]\n",
      "[neutral (0.4301)]\n",
      "[neutral (0.5562)]\n",
      "[neutral (0.3695)]\n",
      "[neutral (0.5655)]\n",
      "[bullish (0.6402)]\n",
      "[neutral (0.5489)]\n",
      "[neutral (0.5162)]\n",
      "[neutral (0.613)]\n",
      "[neutral (0.4232)]\n",
      "[bullish (0.5929)]\n",
      "[neutral (0.673)]\n",
      "[neutral (0.4175)]\n",
      "[bearish (0.4719)]\n",
      "[neutral (0.3863)]\n",
      "[bearish (0.3806)]\n",
      "[bullish (0.3816)]\n",
      "[neutral (0.3807)]\n",
      "[neutral (0.49)]\n",
      "[neutral (0.4519)]\n",
      "[bearish (0.3984)]\n",
      "[bullish (0.3869)]\n",
      "[neutral (0.4473)]\n",
      "[bullish (0.4188)]\n",
      "[neutral (0.464)]\n",
      "[neutral (0.3873)]\n",
      "[bullish (0.4625)]\n",
      "[neutral (0.6033)]\n",
      "[bullish (0.6122)]\n",
      "[neutral (0.4987)]\n",
      "[bearish (0.3941)]\n",
      "[neutral (0.5009)]\n",
      "[neutral (0.5314)]\n",
      "[neutral (0.5441)]\n",
      "[neutral (0.6957)]\n",
      "[neutral (0.5326)]\n",
      "[neutral (0.4076)]\n",
      "[neutral (0.5761)]\n",
      "[bullish (0.4478)]\n",
      "[bearish (0.3727)]\n",
      "[neutral (0.5106)]\n",
      "[neutral (0.4879)]\n",
      "[neutral (0.3864)]\n",
      "[bullish (0.3793)]\n",
      "[bullish (0.4978)]\n",
      "[neutral (0.4003)]\n",
      "[neutral (0.4349)]\n",
      "[neutral (0.639)]\n",
      "[bullish (0.4027)]\n",
      "[bullish (0.4216)]\n",
      "[bullish (0.4111)]\n",
      "[bullish (0.5652)]\n",
      "[bullish (0.5592)]\n",
      "[bullish (0.4893)]\n",
      "[bullish (0.5079)]\n",
      "[neutral (0.4052)]\n",
      "[neutral (0.4467)]\n",
      "[neutral (0.4309)]\n",
      "[bullish (0.6099)]\n",
      "[bearish (0.3666)]\n",
      "[bullish (0.5441)]\n",
      "[neutral (0.3589)]\n",
      "[bullish (0.4863)]\n",
      "[bearish (0.3531)]\n",
      "[bullish (0.5531)]\n",
      "[neutral (0.3884)]\n",
      "[bullish (0.4592)]\n",
      "[bullish (0.4416)]\n",
      "[neutral (0.3518)]\n",
      "[neutral (0.4534)]\n",
      "[bearish (0.4855)]\n",
      "[neutral (0.4127)]\n",
      "[bullish (0.4737)]\n",
      "[bearish (0.3432)]\n",
      "[neutral (0.3744)]\n",
      "[bullish (0.6375)]\n",
      "4\n",
      "[neutral (0.5015)]\n",
      "[neutral (0.7323)]\n",
      "[neutral (0.6413)]\n",
      "[neutral (0.4144)]\n",
      "[neutral (0.4915)]\n",
      "[neutral (0.5585)]\n",
      "[neutral (0.6854)]\n",
      "5\n",
      "[neutral (0.6396)]\n",
      "[neutral (0.5675)]\n",
      "[neutral (0.4105)]\n",
      "[neutral (0.4707)]\n",
      "[neutral (0.5895)]\n",
      "[neutral (0.561)]\n",
      "[neutral (0.4071)]\n",
      "[bearish (0.3845)]\n",
      "[neutral (0.5228)]\n",
      "[neutral (0.43)]\n",
      "[neutral (0.4512)]\n",
      "[neutral (0.6068)]\n",
      "[bullish (0.4585)]\n",
      "[neutral (0.7578)]\n",
      "[bearish (0.3382)]\n",
      "[bullish (0.3588)]\n",
      "[neutral (0.5869)]\n",
      "[bullish (0.4202)]\n",
      "[neutral (0.3925)]\n",
      "[neutral (0.5385)]\n",
      "[neutral (0.4594)]\n",
      "[bullish (0.5954)]\n",
      "[neutral (0.5273)]\n",
      "[bearish (0.434)]\n",
      "[bullish (0.4777)]\n",
      "[neutral (0.5666)]\n",
      "[neutral (0.4098)]\n",
      "[bearish (0.459)]\n",
      "[neutral (0.3995)]\n",
      "[neutral (0.5629)]\n",
      "[neutral (0.4082)]\n",
      "[neutral (0.4242)]\n",
      "[neutral (0.4558)]\n",
      "[bearish (0.3657)]\n",
      "6\n",
      "[neutral (0.4798)]\n",
      "[bullish (0.5001)]\n",
      "[bearish (0.4352)]\n",
      "[neutral (0.4169)]\n",
      "[neutral (0.4643)]\n",
      "[neutral (0.6089)]\n",
      "[neutral (0.5701)]\n",
      "[neutral (0.6848)]\n",
      "[bullish (0.4406)]\n",
      "[neutral (0.5351)]\n",
      "[neutral (0.5516)]\n",
      "[neutral (0.638)]\n",
      "[neutral (0.4832)]\n",
      "[neutral (0.4209)]\n",
      "[neutral (0.4671)]\n",
      "[bullish (0.3813)]\n",
      "[neutral (0.4003)]\n",
      "[neutral (0.5771)]\n",
      "[bullish (0.3991)]\n",
      "[neutral (0.6276)]\n",
      "[bearish (0.4142)]\n",
      "[bullish (0.6847)]\n",
      "7\n",
      "[neutral (0.5452)]\n",
      "[bullish (0.3626)]\n",
      "[neutral (0.5561)]\n",
      "[neutral (0.4225)]\n",
      "[bearish (0.3911)]\n",
      "[neutral (0.5374)]\n",
      "[neutral (0.3768)]\n",
      "[neutral (0.577)]\n",
      "[neutral (0.4296)]\n",
      "[neutral (0.5501)]\n",
      "[bullish (0.4928)]\n",
      "8\n",
      "[neutral (0.4845)]\n",
      "[neutral (0.5872)]\n",
      "[neutral (0.377)]\n",
      "[neutral (0.3783)]\n",
      "[neutral (0.5065)]\n",
      "[neutral (0.5011)]\n",
      "9\n",
      "[neutral (0.4329)]\n",
      "[neutral (0.604)]\n",
      "[neutral (0.7391)]\n",
      "[neutral (0.5858)]\n",
      "[neutral (0.6086)]\n",
      "[neutral (0.6549)]\n",
      "[neutral (0.6231)]\n",
      "[neutral (0.4761)]\n",
      "[neutral (0.5962)]\n",
      "[neutral (0.5062)]\n",
      "[neutral (0.6003)]\n",
      "[neutral (0.6908)]\n",
      "[neutral (0.5828)]\n",
      "[bullish (0.4617)]\n",
      "[neutral (0.502)]\n",
      "[bullish (0.3921)]\n",
      "[neutral (0.7517)]\n",
      "[neutral (0.4407)]\n",
      "[neutral (0.7458)]\n",
      "10\n",
      "[neutral (0.4006)]\n",
      "[neutral (0.4305)]\n",
      "11\n",
      "[neutral (0.3837)]\n",
      "[neutral (0.3777)]\n",
      "[neutral (0.3694)]\n",
      "[neutral (0.5381)]\n",
      "12\n",
      "[neutral (0.4074)]\n",
      "[neutral (0.3888)]\n",
      "[bearish (0.4059)]\n",
      "[neutral (0.6858)]\n",
      "[neutral (0.3939)]\n",
      "[neutral (0.4234)]\n",
      "[neutral (0.4108)]\n",
      "[neutral (0.4728)]\n",
      "[bullish (0.4416)]\n",
      "[bullish (0.5405)]\n",
      "[bearish (0.4135)]\n",
      "[neutral (0.4797)]\n",
      "[neutral (0.3957)]\n",
      "[neutral (0.4854)]\n",
      "[bullish (0.6977)]\n",
      "13\n",
      "[bullish (0.454)]\n",
      "14\n",
      "[neutral (0.6419)]\n",
      "[bearish (0.4403)]\n",
      "[bearish (0.4318)]\n",
      "[bullish (0.5894)]\n",
      "15\n",
      "[neutral (0.4691)]\n",
      "[neutral (0.5432)]\n",
      "[bullish (0.4033)]\n",
      "[neutral (0.4169)]\n",
      "[neutral (0.3742)]\n",
      "[neutral (0.5411)]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d70660e4be8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/models/text_classification_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, mini_batch_size, multi_class_prob, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/models/text_classification_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0membedding_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/base.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0meverything_embedded\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/document.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# embed words in the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mlengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/token.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/base.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0meverything_embedded\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/token.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;31m# get hidden states from language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m             all_hidden_states_in_lm = self.lm.get_representation(\n\u001b[0m\u001b[1;32m    610\u001b[0m                 \u001b[0mtext_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_marker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_marker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchars_per_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/models/language_model.py\u001b[0m in \u001b[0;36mget_representation\u001b[0;34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0moutput_parts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/models/language_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, ordered_sequence_lengths)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    577\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in df['Body']:\n",
    "    print(count)\n",
    "    doc = i\n",
    "    strings = doc.split('. ')\n",
    "    for sent in strings:\n",
    "        sentence = Sentence(sent)\n",
    "        classifier.predict(sentence)\n",
    "        print(sentence.labels)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "sentence.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}