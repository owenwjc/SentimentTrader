{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('vsenv': conda)",
   "display_name": "Python 3.8.5 64-bit ('vsenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1ee30a202add5215adcbb767012aec8477a9c484e522cef3494d0f72d70b8c03"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import nltk\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from bson.objectid import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost',27017)\n",
    "db = client.db\n",
    "comments = db.comments\n",
    "companies = db.companylist\n",
    "matches = db.matches\n",
    "chunks = db.chunks\n",
    "\n",
    "df = pd.DataFrame.from_records(comments.find({'Label':{'$ne': 0}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"nltk.download('vader_lexicon')\\nsir = SentimentIntensityAnalyzer()\\nsent = df.loc[77,'Title'] + ' ' + df.loc[77,'Body']\\nsir.polarity_scores(sent)['compound']\\ncount = 0\\n\\nfor i in range(len(df)):\\n    sent = df.loc[i,'Title'] + ' ' + df.loc[i,'Body']\\n    score = sir.polarity_scores(sent)['compound']\\n    if (score < 0 and df.loc[i, 'Label'] == 'bullish') or (score > 0 and df.loc[i,'Label'] == 'bearish'):\\n        print(str(score) + ':' + df.loc[i, 'Label'])\\n        count = count + 1\""
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "'''nltk.download('vader_lexicon')\n",
    "sir = SentimentIntensityAnalyzer()\n",
    "sent = df.loc[77,'Title'] + ' ' + df.loc[77,'Body']\n",
    "sir.polarity_scores(sent)['compound']\n",
    "count = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    sent = df.loc[i,'Title'] + ' ' + df.loc[i,'Body']\n",
    "    score = sir.polarity_scores(sent)['compound']\n",
    "    if (score < 0 and df.loc[i, 'Label'] == 'bullish') or (score > 0 and df.loc[i,'Label'] == 'bearish'):\n",
    "        print(str(score) + ':' + df.loc[i, 'Label'])\n",
    "        count = count + 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf = pd.DataFrame(columns = ['Label', 'Text'])\n",
    "labelDf['Text'] = df['Body']\n",
    "labelDf['Label'] = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf['Label'] = '__label__' + labelDf['Label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf = labelDf.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf.iloc[0: int(len(labelDf)*0.8)].to_csv('data/train.csv', sep = '\\t', index = False, header = False)\n",
    "labelDf.iloc[int(len(labelDf)*0.8): int(len(labelDf)*0.9)].to_csv('data/test.csv', sep = '\\t', index = False, header = False)\n",
    "labelDf.iloc[int(len(labelDf)*0.9): ].to_csv('data/dev.csv', sep = '\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-11-03 11:10:21,966 Reading data from data\n",
      "2020-11-03 11:10:21,968 Train: data/train.csv\n",
      "2020-11-03 11:10:21,968 Dev: data/dev.csv\n",
      "2020-11-03 11:10:21,969 Test: data/test.csv\n",
      "2020-11-03 11:10:23,750 Computing label dictionary. Progress:\n",
      "100%|██████████| 3601/3601 [00:01<00:00, 3028.73it/s]2020-11-03 11:10:25,864 [b'neutral', b'bearish', b'bullish']\n",
      "2020-11-03 11:10:25,874 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:10:25,878 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-11-03 11:10:25,880 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:10:25,881 Corpus: \"Corpus: 3201 train + 401 dev + 400 test sentences\"\n",
      "2020-11-03 11:10:25,884 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:10:25,887 Parameters:\n",
      "2020-11-03 11:10:25,888  - learning_rate: \"0.1\"\n",
      "2020-11-03 11:10:25,889  - mini_batch_size: \"32\"\n",
      "2020-11-03 11:10:25,890  - patience: \"3\"\n",
      "2020-11-03 11:10:25,891  - anneal_factor: \"0.5\"\n",
      "2020-11-03 11:10:25,892  - max_epochs: \"10\"\n",
      "2020-11-03 11:10:25,893  - shuffle: \"True\"\n",
      "2020-11-03 11:10:25,897  - train_with_dev: \"False\"\n",
      "2020-11-03 11:10:25,899  - batch_growth_annealing: \"False\"\n",
      "2020-11-03 11:10:25,900 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:10:25,904 Model training base path: \"model1\"\n",
      "2020-11-03 11:10:25,907 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:10:25,908 Device: cpu\n",
      "2020-11-03 11:10:25,908 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:10:25,911 Embeddings storage mode: cpu\n",
      "2020-11-03 11:10:25,913 ----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2020-11-03 11:10:48,284 epoch 1 - iter 10/101 - loss 1.14313285 - samples/sec: 14.95 - lr: 0.100000\n",
      "2020-11-03 11:11:07,235 epoch 1 - iter 20/101 - loss 1.12136025 - samples/sec: 16.90 - lr: 0.100000\n",
      "2020-11-03 11:11:42,876 epoch 1 - iter 30/101 - loss 1.11414674 - samples/sec: 8.99 - lr: 0.100000\n",
      "2020-11-03 11:12:02,373 epoch 1 - iter 40/101 - loss 1.11388690 - samples/sec: 16.42 - lr: 0.100000\n",
      "2020-11-03 11:12:23,508 epoch 1 - iter 50/101 - loss 1.10722764 - samples/sec: 15.16 - lr: 0.100000\n",
      "2020-11-03 11:12:47,494 epoch 1 - iter 60/101 - loss 1.10200596 - samples/sec: 13.36 - lr: 0.100000\n",
      "2020-11-03 11:13:17,851 epoch 1 - iter 70/101 - loss 1.09930370 - samples/sec: 10.55 - lr: 0.100000\n",
      "2020-11-03 11:14:00,760 epoch 1 - iter 80/101 - loss 1.09758649 - samples/sec: 7.46 - lr: 0.100000\n",
      "2020-11-03 11:14:21,395 epoch 1 - iter 90/101 - loss 1.09699736 - samples/sec: 15.52 - lr: 0.100000\n",
      "2020-11-03 11:14:51,294 epoch 1 - iter 100/101 - loss 1.09608743 - samples/sec: 10.70 - lr: 0.100000\n",
      "2020-11-03 11:14:51,536 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:14:51,537 EPOCH 1 done: loss 1.0965 - lr 0.1000000\n",
      "2020-11-03 11:15:19,598 DEV : loss 1.242918610572815 - score 0.3192\n",
      "2020-11-03 11:15:19,852 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-03 11:15:22,765 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:15:45,742 epoch 2 - iter 10/101 - loss 1.06886047 - samples/sec: 15.27 - lr: 0.100000\n",
      "2020-11-03 11:16:08,171 epoch 2 - iter 20/101 - loss 1.06642728 - samples/sec: 14.28 - lr: 0.100000\n",
      "2020-11-03 11:16:43,956 epoch 2 - iter 30/101 - loss 1.07377200 - samples/sec: 8.95 - lr: 0.100000\n",
      "2020-11-03 11:17:13,327 epoch 2 - iter 40/101 - loss 1.07039703 - samples/sec: 11.23 - lr: 0.100000\n",
      "2020-11-03 11:17:31,081 epoch 2 - iter 50/101 - loss 1.07627107 - samples/sec: 18.03 - lr: 0.100000\n",
      "2020-11-03 11:17:57,357 epoch 2 - iter 60/101 - loss 1.07276170 - samples/sec: 12.19 - lr: 0.100000\n",
      "2020-11-03 11:18:37,937 epoch 2 - iter 70/101 - loss 1.06872275 - samples/sec: 7.89 - lr: 0.100000\n",
      "2020-11-03 11:18:55,719 epoch 2 - iter 80/101 - loss 1.06910855 - samples/sec: 18.01 - lr: 0.100000\n",
      "2020-11-03 11:19:12,186 epoch 2 - iter 90/101 - loss 1.06795211 - samples/sec: 19.46 - lr: 0.100000\n",
      "2020-11-03 11:19:34,540 epoch 2 - iter 100/101 - loss 1.07094118 - samples/sec: 14.33 - lr: 0.100000\n",
      "2020-11-03 11:19:34,824 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:19:34,889 EPOCH 2 done: loss 1.0713 - lr 0.1000000\n",
      "2020-11-03 11:20:05,103 DEV : loss 1.1336480379104614 - score 0.3741\n",
      "2020-11-03 11:20:05,348 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-03 11:20:08,317 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:20:41,297 epoch 3 - iter 10/101 - loss 1.06573174 - samples/sec: 10.94 - lr: 0.100000\n",
      "2020-11-03 11:20:59,314 epoch 3 - iter 20/101 - loss 1.06834031 - samples/sec: 17.77 - lr: 0.100000\n",
      "2020-11-03 11:21:20,751 epoch 3 - iter 30/101 - loss 1.06221365 - samples/sec: 14.94 - lr: 0.100000\n",
      "2020-11-03 11:21:41,558 epoch 3 - iter 40/101 - loss 1.06714149 - samples/sec: 15.39 - lr: 0.100000\n",
      "2020-11-03 11:21:59,210 epoch 3 - iter 50/101 - loss 1.06405002 - samples/sec: 18.16 - lr: 0.100000\n",
      "2020-11-03 11:22:22,999 epoch 3 - iter 60/101 - loss 1.06142406 - samples/sec: 13.47 - lr: 0.100000\n",
      "2020-11-03 11:22:52,037 epoch 3 - iter 70/101 - loss 1.06329955 - samples/sec: 11.03 - lr: 0.100000\n",
      "2020-11-03 11:23:06,934 epoch 3 - iter 80/101 - loss 1.06333698 - samples/sec: 21.52 - lr: 0.100000\n",
      "2020-11-03 11:23:28,173 epoch 3 - iter 90/101 - loss 1.06389227 - samples/sec: 15.54 - lr: 0.100000\n",
      "2020-11-03 11:24:02,279 epoch 3 - iter 100/101 - loss 1.06273083 - samples/sec: 9.39 - lr: 0.100000\n",
      "2020-11-03 11:24:02,518 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:24:02,584 EPOCH 3 done: loss 1.0599 - lr 0.1000000\n",
      "2020-11-03 11:24:29,151 DEV : loss 1.2313272953033447 - score 0.3815\n",
      "2020-11-03 11:24:29,422 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-03 11:24:32,491 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:24:57,915 epoch 4 - iter 10/101 - loss 1.07986379 - samples/sec: 13.79 - lr: 0.100000\n",
      "2020-11-03 11:25:13,085 epoch 4 - iter 20/101 - loss 1.07639943 - samples/sec: 21.12 - lr: 0.100000\n",
      "2020-11-03 11:25:45,673 epoch 4 - iter 30/101 - loss 1.06509195 - samples/sec: 9.83 - lr: 0.100000\n",
      "2020-11-03 11:26:01,603 epoch 4 - iter 40/101 - loss 1.06343580 - samples/sec: 20.11 - lr: 0.100000\n",
      "2020-11-03 11:26:36,864 epoch 4 - iter 50/101 - loss 1.05774140 - samples/sec: 9.29 - lr: 0.100000\n",
      "2020-11-03 11:26:51,026 epoch 4 - iter 60/101 - loss 1.05880083 - samples/sec: 22.62 - lr: 0.100000\n",
      "2020-11-03 11:27:08,165 epoch 4 - iter 70/101 - loss 1.06082746 - samples/sec: 18.68 - lr: 0.100000\n",
      "2020-11-03 11:27:30,596 epoch 4 - iter 80/101 - loss 1.05960676 - samples/sec: 14.28 - lr: 0.100000\n",
      "2020-11-03 11:27:59,618 epoch 4 - iter 90/101 - loss 1.06056788 - samples/sec: 11.03 - lr: 0.100000\n",
      "2020-11-03 11:28:16,193 epoch 4 - iter 100/101 - loss 1.05567727 - samples/sec: 19.31 - lr: 0.100000\n",
      "2020-11-03 11:28:16,512 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:28:16,639 EPOCH 4 done: loss 1.0573 - lr 0.1000000\n",
      "2020-11-03 11:28:44,163 DEV : loss 1.2693276405334473 - score 0.3716\n",
      "2020-11-03 11:28:44,404 BAD EPOCHS (no improvement): 1\n",
      "2020-11-03 11:28:44,406 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:29:08,529 epoch 5 - iter 10/101 - loss 1.04235310 - samples/sec: 16.29 - lr: 0.100000\n",
      "2020-11-03 11:29:31,555 epoch 5 - iter 20/101 - loss 1.05170245 - samples/sec: 14.36 - lr: 0.100000\n",
      "2020-11-03 11:29:53,584 epoch 5 - iter 30/101 - loss 1.05117419 - samples/sec: 14.54 - lr: 0.100000\n",
      "2020-11-03 11:30:18,645 epoch 5 - iter 40/101 - loss 1.04856890 - samples/sec: 12.77 - lr: 0.100000\n",
      "2020-11-03 11:30:38,234 epoch 5 - iter 50/101 - loss 1.05250371 - samples/sec: 16.35 - lr: 0.100000\n",
      "2020-11-03 11:31:15,371 epoch 5 - iter 60/101 - loss 1.05194683 - samples/sec: 8.62 - lr: 0.100000\n",
      "2020-11-03 11:31:35,379 epoch 5 - iter 70/101 - loss 1.05349477 - samples/sec: 16.00 - lr: 0.100000\n",
      "2020-11-03 11:32:09,844 epoch 5 - iter 80/101 - loss 1.05013644 - samples/sec: 9.29 - lr: 0.100000\n",
      "2020-11-03 11:32:25,292 epoch 5 - iter 90/101 - loss 1.04914440 - samples/sec: 21.93 - lr: 0.100000\n",
      "2020-11-03 11:32:43,165 epoch 5 - iter 100/101 - loss 1.04693860 - samples/sec: 17.92 - lr: 0.100000\n",
      "2020-11-03 11:32:43,379 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:32:43,406 EPOCH 5 done: loss 1.0398 - lr 0.1000000\n",
      "2020-11-03 11:33:10,478 DEV : loss 1.2042795419692993 - score 0.3342\n",
      "2020-11-03 11:33:10,720 BAD EPOCHS (no improvement): 2\n",
      "2020-11-03 11:33:10,722 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:33:42,858 epoch 6 - iter 10/101 - loss 1.02081091 - samples/sec: 10.64 - lr: 0.100000\n",
      "2020-11-03 11:33:59,984 epoch 6 - iter 20/101 - loss 1.02147048 - samples/sec: 18.70 - lr: 0.100000\n",
      "2020-11-03 11:34:23,452 epoch 6 - iter 30/101 - loss 1.03332723 - samples/sec: 13.65 - lr: 0.100000\n",
      "2020-11-03 11:34:50,872 epoch 6 - iter 40/101 - loss 1.03105117 - samples/sec: 11.68 - lr: 0.100000\n",
      "2020-11-03 11:35:09,634 epoch 6 - iter 50/101 - loss 1.03366918 - samples/sec: 17.07 - lr: 0.100000\n",
      "2020-11-03 11:35:26,809 epoch 6 - iter 60/101 - loss 1.03318665 - samples/sec: 19.48 - lr: 0.100000\n",
      "2020-11-03 11:35:43,417 epoch 6 - iter 70/101 - loss 1.03634375 - samples/sec: 19.29 - lr: 0.100000\n",
      "2020-11-03 11:36:18,140 epoch 6 - iter 80/101 - loss 1.03512920 - samples/sec: 9.22 - lr: 0.100000\n",
      "2020-11-03 11:36:38,683 epoch 6 - iter 90/101 - loss 1.03795160 - samples/sec: 15.60 - lr: 0.100000\n",
      "2020-11-03 11:36:57,798 epoch 6 - iter 100/101 - loss 1.03958544 - samples/sec: 16.75 - lr: 0.100000\n",
      "2020-11-03 11:36:58,185 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:36:58,190 EPOCH 6 done: loss 1.0394 - lr 0.1000000\n",
      "2020-11-03 11:37:25,242 DEV : loss 1.4624158143997192 - score 0.3691\n",
      "2020-11-03 11:37:25,485 BAD EPOCHS (no improvement): 3\n",
      "2020-11-03 11:37:25,487 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:38:08,752 epoch 7 - iter 10/101 - loss 1.07887982 - samples/sec: 7.88 - lr: 0.100000\n",
      "2020-11-03 11:38:34,244 epoch 7 - iter 20/101 - loss 1.07270889 - samples/sec: 12.57 - lr: 0.100000\n",
      "2020-11-03 11:38:52,777 epoch 7 - iter 30/101 - loss 1.06086939 - samples/sec: 18.07 - lr: 0.100000\n",
      "2020-11-03 11:39:11,773 epoch 7 - iter 40/101 - loss 1.05721521 - samples/sec: 16.86 - lr: 0.100000\n",
      "2020-11-03 11:39:28,469 epoch 7 - iter 50/101 - loss 1.05030568 - samples/sec: 19.19 - lr: 0.100000\n",
      "2020-11-03 11:39:50,338 epoch 7 - iter 60/101 - loss 1.04931597 - samples/sec: 14.66 - lr: 0.100000\n",
      "2020-11-03 11:40:09,954 epoch 7 - iter 70/101 - loss 1.04369209 - samples/sec: 16.32 - lr: 0.100000\n",
      "2020-11-03 11:40:33,297 epoch 7 - iter 80/101 - loss 1.03965389 - samples/sec: 13.72 - lr: 0.100000\n",
      "2020-11-03 11:41:04,586 epoch 7 - iter 90/101 - loss 1.03946492 - samples/sec: 10.23 - lr: 0.100000\n",
      "2020-11-03 11:41:21,158 epoch 7 - iter 100/101 - loss 1.04000445 - samples/sec: 19.32 - lr: 0.100000\n",
      "2020-11-03 11:41:21,375 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:41:21,393 EPOCH 7 done: loss 1.0383 - lr 0.1000000\n",
      "2020-11-03 11:41:49,723 DEV : loss 1.532294750213623 - score 0.3666\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-11-03 11:41:49,971 BAD EPOCHS (no improvement): 4\n",
      "2020-11-03 11:41:49,973 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:42:12,715 epoch 8 - iter 10/101 - loss 1.08996285 - samples/sec: 15.36 - lr: 0.050000\n",
      "2020-11-03 11:42:48,619 epoch 8 - iter 20/101 - loss 1.05726206 - samples/sec: 8.92 - lr: 0.050000\n",
      "2020-11-03 11:43:04,839 epoch 8 - iter 30/101 - loss 1.04246286 - samples/sec: 19.76 - lr: 0.050000\n",
      "2020-11-03 11:43:30,186 epoch 8 - iter 40/101 - loss 1.03825119 - samples/sec: 12.64 - lr: 0.050000\n",
      "2020-11-03 11:43:51,604 epoch 8 - iter 50/101 - loss 1.03903896 - samples/sec: 14.95 - lr: 0.050000\n",
      "2020-11-03 11:44:15,639 epoch 8 - iter 60/101 - loss 1.03212550 - samples/sec: 13.33 - lr: 0.050000\n",
      "2020-11-03 11:44:36,232 epoch 8 - iter 70/101 - loss 1.03461316 - samples/sec: 16.18 - lr: 0.050000\n",
      "2020-11-03 11:44:50,526 epoch 8 - iter 80/101 - loss 1.03313599 - samples/sec: 22.41 - lr: 0.050000\n",
      "2020-11-03 11:45:21,016 epoch 8 - iter 90/101 - loss 1.03031148 - samples/sec: 10.50 - lr: 0.050000\n",
      "2020-11-03 11:45:38,817 epoch 8 - iter 100/101 - loss 1.02518692 - samples/sec: 17.99 - lr: 0.050000\n",
      "2020-11-03 11:45:39,071 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:45:39,131 EPOCH 8 done: loss 1.0288 - lr 0.0500000\n",
      "2020-11-03 11:46:05,440 DEV : loss 1.0262091159820557 - score 0.4813\n",
      "2020-11-03 11:46:05,688 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-03 11:46:08,596 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:46:46,680 epoch 9 - iter 10/101 - loss 1.02489844 - samples/sec: 9.00 - lr: 0.050000\n",
      "2020-11-03 11:47:07,545 epoch 9 - iter 20/101 - loss 1.01807643 - samples/sec: 15.35 - lr: 0.050000\n",
      "2020-11-03 11:47:28,952 epoch 9 - iter 30/101 - loss 1.02015465 - samples/sec: 15.53 - lr: 0.050000\n",
      "2020-11-03 11:47:47,857 epoch 9 - iter 40/101 - loss 1.01216443 - samples/sec: 16.94 - lr: 0.050000\n",
      "2020-11-03 11:48:12,608 epoch 9 - iter 50/101 - loss 1.01223571 - samples/sec: 12.93 - lr: 0.050000\n",
      "2020-11-03 11:48:34,829 epoch 9 - iter 60/101 - loss 1.01190651 - samples/sec: 14.41 - lr: 0.050000\n",
      "2020-11-03 11:48:59,086 epoch 9 - iter 70/101 - loss 1.00537816 - samples/sec: 13.20 - lr: 0.050000\n",
      "2020-11-03 11:49:14,697 epoch 9 - iter 80/101 - loss 1.00591318 - samples/sec: 20.51 - lr: 0.050000\n",
      "2020-11-03 11:49:32,914 epoch 9 - iter 90/101 - loss 1.00810222 - samples/sec: 17.59 - lr: 0.050000\n",
      "2020-11-03 11:49:51,399 epoch 9 - iter 100/101 - loss 1.00810733 - samples/sec: 17.32 - lr: 0.050000\n",
      "2020-11-03 11:49:51,629 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:49:51,679 EPOCH 9 done: loss 1.0111 - lr 0.0500000\n",
      "2020-11-03 11:50:21,682 DEV : loss 1.0416325330734253 - score 0.4813\n",
      "2020-11-03 11:50:21,938 BAD EPOCHS (no improvement): 1\n",
      "2020-11-03 11:50:21,941 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:50:38,945 epoch 10 - iter 10/101 - loss 0.99035459 - samples/sec: 21.22 - lr: 0.050000\n",
      "2020-11-03 11:50:59,670 epoch 10 - iter 20/101 - loss 1.01420880 - samples/sec: 15.45 - lr: 0.050000\n",
      "2020-11-03 11:51:40,125 epoch 10 - iter 30/101 - loss 1.01390496 - samples/sec: 7.91 - lr: 0.050000\n",
      "2020-11-03 11:51:52,551 epoch 10 - iter 40/101 - loss 1.00711524 - samples/sec: 25.81 - lr: 0.050000\n",
      "2020-11-03 11:52:10,796 epoch 10 - iter 50/101 - loss 1.00701232 - samples/sec: 17.55 - lr: 0.050000\n",
      "2020-11-03 11:52:41,563 epoch 10 - iter 60/101 - loss 1.00697737 - samples/sec: 10.41 - lr: 0.050000\n",
      "2020-11-03 11:52:59,935 epoch 10 - iter 70/101 - loss 1.00618782 - samples/sec: 18.16 - lr: 0.050000\n",
      "2020-11-03 11:53:18,253 epoch 10 - iter 80/101 - loss 1.00700380 - samples/sec: 17.48 - lr: 0.050000\n",
      "2020-11-03 11:53:44,579 epoch 10 - iter 90/101 - loss 1.00417555 - samples/sec: 12.16 - lr: 0.050000\n",
      "2020-11-03 11:54:03,563 epoch 10 - iter 100/101 - loss 1.00523691 - samples/sec: 16.88 - lr: 0.050000\n",
      "2020-11-03 11:54:04,077 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:54:04,168 EPOCH 10 done: loss 1.0115 - lr 0.0500000\n",
      "2020-11-03 11:54:32,585 DEV : loss 1.0657696723937988 - score 0.409\n",
      "2020-11-03 11:54:32,850 BAD EPOCHS (no improvement): 2\n",
      "2020-11-03 11:54:36,072 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-03 11:54:36,073 Testing using best model ...\n",
      "2020-11-03 11:54:36,074 loading file model1/best-model.pt\n",
      "2020-11-03 11:55:05,347 \t0.4775\n",
      "2020-11-03 11:55:05,348 \n",
      "Results:\n",
      "- F-score (micro) 0.4775\n",
      "- F-score (macro) 0.4672\n",
      "- Accuracy 0.4775\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.6702    0.3443    0.4549       183\n",
      "     bearish     0.3504    0.4505    0.3942        91\n",
      "     bullish     0.4603    0.6905    0.5524       126\n",
      "\n",
      "   micro avg     0.4775    0.4775    0.4775       400\n",
      "   macro avg     0.4937    0.4951    0.4672       400\n",
      "weighted avg     0.5313    0.4775    0.4718       400\n",
      " samples avg     0.4775    0.4775    0.4775       400\n",
      "\n",
      "2020-11-03 11:55:05,349 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'test_score': 0.4775,\n",
       " 'dev_score_history': [0.3192,\n",
       "  0.3741,\n",
       "  0.3815,\n",
       "  0.3716,\n",
       "  0.3342,\n",
       "  0.3691,\n",
       "  0.3666,\n",
       "  0.4813,\n",
       "  0.4813,\n",
       "  0.409],\n",
       " 'train_loss_history': [1.0964837605410283,\n",
       "  1.071310033302496,\n",
       "  1.0598636723981045,\n",
       "  1.0573376476174534,\n",
       "  1.0398043665555443,\n",
       "  1.03936900183706,\n",
       "  1.0382626127488543,\n",
       "  1.0288294707194414,\n",
       "  1.0111010529027127,\n",
       "  1.0114670065369937],\n",
       " 'dev_loss_history': [1.242918610572815,\n",
       "  1.1336480379104614,\n",
       "  1.2313272953033447,\n",
       "  1.2693276405334473,\n",
       "  1.2042795419692993,\n",
       "  1.4624158143997192,\n",
       "  1.532294750213623,\n",
       "  1.0262091159820557,\n",
       "  1.0416325330734253,\n",
       "  1.0657696723937988]}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from flair.datasets import ClassificationCorpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "corpus = ClassificationCorpus(Path('data/'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
    "word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('model1/', max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-11-02 18:11:40,431 loading file model/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier.load('model/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'value': 'neutral', 'confidence': 0.46689432859420776}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "sentence = Sentence('stonks are flat')\n",
    "classifier.predict(sentence)\n",
    "sentence.labels[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(db.threads.find({'Label':{'$ne': 0}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "[bullish (0.4264)]\n",
      "[bearish (0.4014)]\n",
      "[bullish (0.3883)]\n",
      "[neutral (0.361)]\n",
      "[neutral (0.5081)]\n",
      "[neutral (0.4587)]\n",
      "[neutral (0.5477)]\n",
      "[neutral (0.6959)]\n",
      "[neutral (0.4369)]\n",
      "1\n",
      "[neutral (0.4986)]\n",
      "[bullish (0.4714)]\n",
      "[neutral (0.3696)]\n",
      "[bullish (0.469)]\n",
      "[bearish (0.4374)]\n",
      "[bullish (0.583)]\n",
      "[neutral (0.5378)]\n",
      "[neutral (0.4533)]\n",
      "[bullish (0.3903)]\n",
      "[neutral (0.4696)]\n",
      "[neutral (0.5345)]\n",
      "[bullish (0.3929)]\n",
      "[bearish (0.4591)]\n",
      "[bearish (0.4687)]\n",
      "[bullish (0.6079)]\n",
      "[bullish (0.5661)]\n",
      "[bullish (0.4594)]\n",
      "[neutral (0.4239)]\n",
      "[bullish (0.3992)]\n",
      "[bullish (0.3883)]\n",
      "[bullish (0.3863)]\n",
      "[neutral (0.4566)]\n",
      "[bearish (0.4039)]\n",
      "[neutral (0.4199)]\n",
      "[neutral (0.6094)]\n",
      "[neutral (0.6532)]\n",
      "[neutral (0.3901)]\n",
      "2\n",
      "[neutral (0.4543)]\n",
      "[neutral (0.4557)]\n",
      "[bullish (0.485)]\n",
      "[bearish (0.4268)]\n",
      "[bullish (0.4372)]\n",
      "[neutral (0.3578)]\n",
      "[neutral (0.6233)]\n",
      "[bearish (0.4631)]\n",
      "[neutral (0.3811)]\n",
      "[neutral (0.5655)]\n",
      "[neutral (0.5073)]\n",
      "[neutral (0.3623)]\n",
      "[neutral (0.5085)]\n",
      "[neutral (0.5124)]\n",
      "[neutral (0.6283)]\n",
      "[neutral (0.4525)]\n",
      "3\n",
      "[bullish (0.542)]\n",
      "[neutral (0.5325)]\n",
      "[neutral (0.3989)]\n",
      "[bullish (0.3428)]\n",
      "[neutral (0.7404)]\n",
      "[neutral (0.5306)]\n",
      "[neutral (0.6469)]\n",
      "[neutral (0.4649)]\n",
      "[neutral (0.4691)]\n",
      "[bullish (0.3546)]\n",
      "[bullish (0.3604)]\n",
      "[neutral (0.3904)]\n",
      "[neutral (0.4301)]\n",
      "[neutral (0.5562)]\n",
      "[neutral (0.3695)]\n",
      "[neutral (0.5655)]\n",
      "[bullish (0.6402)]\n",
      "[neutral (0.5489)]\n",
      "[neutral (0.5162)]\n",
      "[neutral (0.613)]\n",
      "[neutral (0.4232)]\n",
      "[bullish (0.5929)]\n",
      "[neutral (0.673)]\n",
      "[neutral (0.4175)]\n",
      "[bearish (0.4719)]\n",
      "[neutral (0.3863)]\n",
      "[bearish (0.3806)]\n",
      "[bullish (0.3816)]\n",
      "[neutral (0.3807)]\n",
      "[neutral (0.49)]\n",
      "[neutral (0.4519)]\n",
      "[bearish (0.3984)]\n",
      "[bullish (0.3869)]\n",
      "[neutral (0.4473)]\n",
      "[bullish (0.4188)]\n",
      "[neutral (0.464)]\n",
      "[neutral (0.3873)]\n",
      "[bullish (0.4625)]\n",
      "[neutral (0.6033)]\n",
      "[bullish (0.6122)]\n",
      "[neutral (0.4987)]\n",
      "[bearish (0.3941)]\n",
      "[neutral (0.5009)]\n",
      "[neutral (0.5314)]\n",
      "[neutral (0.5441)]\n",
      "[neutral (0.6957)]\n",
      "[neutral (0.5326)]\n",
      "[neutral (0.4076)]\n",
      "[neutral (0.5761)]\n",
      "[bullish (0.4478)]\n",
      "[bearish (0.3727)]\n",
      "[neutral (0.5106)]\n",
      "[neutral (0.4879)]\n",
      "[neutral (0.3864)]\n",
      "[bullish (0.3793)]\n",
      "[bullish (0.4978)]\n",
      "[neutral (0.4003)]\n",
      "[neutral (0.4349)]\n",
      "[neutral (0.639)]\n",
      "[bullish (0.4027)]\n",
      "[bullish (0.4216)]\n",
      "[bullish (0.4111)]\n",
      "[bullish (0.5652)]\n",
      "[bullish (0.5592)]\n",
      "[bullish (0.4893)]\n",
      "[bullish (0.5079)]\n",
      "[neutral (0.4052)]\n",
      "[neutral (0.4467)]\n",
      "[neutral (0.4309)]\n",
      "[bullish (0.6099)]\n",
      "[bearish (0.3666)]\n",
      "[bullish (0.5441)]\n",
      "[neutral (0.3589)]\n",
      "[bullish (0.4863)]\n",
      "[bearish (0.3531)]\n",
      "[bullish (0.5531)]\n",
      "[neutral (0.3884)]\n",
      "[bullish (0.4592)]\n",
      "[bullish (0.4416)]\n",
      "[neutral (0.3518)]\n",
      "[neutral (0.4534)]\n",
      "[bearish (0.4855)]\n",
      "[neutral (0.4127)]\n",
      "[bullish (0.4737)]\n",
      "[bearish (0.3432)]\n",
      "[neutral (0.3744)]\n",
      "[bullish (0.6375)]\n",
      "4\n",
      "[neutral (0.5015)]\n",
      "[neutral (0.7323)]\n",
      "[neutral (0.6413)]\n",
      "[neutral (0.4144)]\n",
      "[neutral (0.4915)]\n",
      "[neutral (0.5585)]\n",
      "[neutral (0.6854)]\n",
      "5\n",
      "[neutral (0.6396)]\n",
      "[neutral (0.5675)]\n",
      "[neutral (0.4105)]\n",
      "[neutral (0.4707)]\n",
      "[neutral (0.5895)]\n",
      "[neutral (0.561)]\n",
      "[neutral (0.4071)]\n",
      "[bearish (0.3845)]\n",
      "[neutral (0.5228)]\n",
      "[neutral (0.43)]\n",
      "[neutral (0.4512)]\n",
      "[neutral (0.6068)]\n",
      "[bullish (0.4585)]\n",
      "[neutral (0.7578)]\n",
      "[bearish (0.3382)]\n",
      "[bullish (0.3588)]\n",
      "[neutral (0.5869)]\n",
      "[bullish (0.4202)]\n",
      "[neutral (0.3925)]\n",
      "[neutral (0.5385)]\n",
      "[neutral (0.4594)]\n",
      "[bullish (0.5954)]\n",
      "[neutral (0.5273)]\n",
      "[bearish (0.434)]\n",
      "[bullish (0.4777)]\n",
      "[neutral (0.5666)]\n",
      "[neutral (0.4098)]\n",
      "[bearish (0.459)]\n",
      "[neutral (0.3995)]\n",
      "[neutral (0.5629)]\n",
      "[neutral (0.4082)]\n",
      "[neutral (0.4242)]\n",
      "[neutral (0.4558)]\n",
      "[bearish (0.3657)]\n",
      "6\n",
      "[neutral (0.4798)]\n",
      "[bullish (0.5001)]\n",
      "[bearish (0.4352)]\n",
      "[neutral (0.4169)]\n",
      "[neutral (0.4643)]\n",
      "[neutral (0.6089)]\n",
      "[neutral (0.5701)]\n",
      "[neutral (0.6848)]\n",
      "[bullish (0.4406)]\n",
      "[neutral (0.5351)]\n",
      "[neutral (0.5516)]\n",
      "[neutral (0.638)]\n",
      "[neutral (0.4832)]\n",
      "[neutral (0.4209)]\n",
      "[neutral (0.4671)]\n",
      "[bullish (0.3813)]\n",
      "[neutral (0.4003)]\n",
      "[neutral (0.5771)]\n",
      "[bullish (0.3991)]\n",
      "[neutral (0.6276)]\n",
      "[bearish (0.4142)]\n",
      "[bullish (0.6847)]\n",
      "7\n",
      "[neutral (0.5452)]\n",
      "[bullish (0.3626)]\n",
      "[neutral (0.5561)]\n",
      "[neutral (0.4225)]\n",
      "[bearish (0.3911)]\n",
      "[neutral (0.5374)]\n",
      "[neutral (0.3768)]\n",
      "[neutral (0.577)]\n",
      "[neutral (0.4296)]\n",
      "[neutral (0.5501)]\n",
      "[bullish (0.4928)]\n",
      "8\n",
      "[neutral (0.4845)]\n",
      "[neutral (0.5872)]\n",
      "[neutral (0.377)]\n",
      "[neutral (0.3783)]\n",
      "[neutral (0.5065)]\n",
      "[neutral (0.5011)]\n",
      "9\n",
      "[neutral (0.4329)]\n",
      "[neutral (0.604)]\n",
      "[neutral (0.7391)]\n",
      "[neutral (0.5858)]\n",
      "[neutral (0.6086)]\n",
      "[neutral (0.6549)]\n",
      "[neutral (0.6231)]\n",
      "[neutral (0.4761)]\n",
      "[neutral (0.5962)]\n",
      "[neutral (0.5062)]\n",
      "[neutral (0.6003)]\n",
      "[neutral (0.6908)]\n",
      "[neutral (0.5828)]\n",
      "[bullish (0.4617)]\n",
      "[neutral (0.502)]\n",
      "[bullish (0.3921)]\n",
      "[neutral (0.7517)]\n",
      "[neutral (0.4407)]\n",
      "[neutral (0.7458)]\n",
      "10\n",
      "[neutral (0.4006)]\n",
      "[neutral (0.4305)]\n",
      "11\n",
      "[neutral (0.3837)]\n",
      "[neutral (0.3777)]\n",
      "[neutral (0.3694)]\n",
      "[neutral (0.5381)]\n",
      "12\n",
      "[neutral (0.4074)]\n",
      "[neutral (0.3888)]\n",
      "[bearish (0.4059)]\n",
      "[neutral (0.6858)]\n",
      "[neutral (0.3939)]\n",
      "[neutral (0.4234)]\n",
      "[neutral (0.4108)]\n",
      "[neutral (0.4728)]\n",
      "[bullish (0.4416)]\n",
      "[bullish (0.5405)]\n",
      "[bearish (0.4135)]\n",
      "[neutral (0.4797)]\n",
      "[neutral (0.3957)]\n",
      "[neutral (0.4854)]\n",
      "[bullish (0.6977)]\n",
      "13\n",
      "[bullish (0.454)]\n",
      "14\n",
      "[neutral (0.6419)]\n",
      "[bearish (0.4403)]\n",
      "[bearish (0.4318)]\n",
      "[bullish (0.5894)]\n",
      "15\n",
      "[neutral (0.4691)]\n",
      "[neutral (0.5432)]\n",
      "[bullish (0.4033)]\n",
      "[neutral (0.4169)]\n",
      "[neutral (0.3742)]\n",
      "[neutral (0.5411)]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d70660e4be8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/models/text_classification_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, mini_batch_size, multi_class_prob, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/models/text_classification_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0membedding_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/base.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0meverything_embedded\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/document.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# embed words in the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mlengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/token.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/base.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0meverything_embedded\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/embeddings/token.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;31m# get hidden states from language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m             all_hidden_states_in_lm = self.lm.get_representation(\n\u001b[0m\u001b[1;32m    610\u001b[0m                 \u001b[0mtext_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_marker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_marker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchars_per_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/models/language_model.py\u001b[0m in \u001b[0;36mget_representation\u001b[0;34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0moutput_parts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/flair/models/language_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, ordered_sequence_lengths)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vsenv/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    577\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in df['Body']:\n",
    "    print(count)\n",
    "    doc = i\n",
    "    strings = doc.split('. ')\n",
    "    for sent in strings:\n",
    "        sentence = Sentence(sent)\n",
    "        classifier.predict(sentence)\n",
    "        print(sentence.labels)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "sentence.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}